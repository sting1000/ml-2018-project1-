{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from functions import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    \"\"\"Standardize the original data set.\"\"\"\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x, mean_x, std_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss.\n",
    "    \"\"\"\n",
    "    e = y - np.tanh(tx.dot(w))\n",
    "\n",
    "    return 1/2*np.mean(e**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    # ***************************************************\n",
    "    # returns weights, loss\n",
    "    # ***************************************************\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    w = np.linalg.solve(a, b)\n",
    "    loss = compute_loss(y, tx, w)\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "        split the dataset based on the split ratio. If ratio is 0.8\n",
    "        you will have 80% of your data set dedicated to training\n",
    "        and the rest dedicated to testing\n",
    "        \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    num = len(y)\n",
    "    order = np.random.permutation(num)\n",
    "    order_1 = order[:int(np.floor(ratio*num))]\n",
    "    order_2 = order[int(np.floor(ratio*num)):num]\n",
    "    x_train = x[order_1]\n",
    "    y_train = y[order_1]\n",
    "    #ids_train = ids[order_1]\n",
    "    x_test = x[order_2]\n",
    "    y_test = y[order_2]\n",
    "    #ids_test = ids[order_2]\n",
    "    return x_train, x_test, y_train, y_test,# ids_train, ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    # ***************************************************\n",
    "    # this function should return the matrix formed\n",
    "    # by applying the polynomial basis to the input data\n",
    "    # ***************************************************\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_predicted_labels(x, w):\n",
    "    y_pred = x.dot(w)\n",
    "    y_pred[np.where(y_pred <= 0)] = -1\n",
    "    y_pred[np.where(y_pred > 0)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(predict_labels, x, y, train=True):\n",
    "    total_correct_labels = np.sum(predict_labels == y)\n",
    "    print('Total correct labels in training: {}'.format(total_correct_labels))\n",
    "    if train:\n",
    "        print('Training accuracy: {}'.format((total_correct_labels / x.shape[0]) * 100))\n",
    "    else:\n",
    "        print('Testing accuracy: {}'.format((total_correct_labels / x.shape[0]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predic(n):\n",
    "    n = np.tanh(n)\n",
    "    n[n>=0] = 1\n",
    "    n[n<0] = -1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    # ***************************************************\n",
    "    #  compute gradient and loss\n",
    "    # ***************************************************\n",
    "    N = len(y)\n",
    "    error = y - predic(np.dot(tx, w))\n",
    "    gradient = -1/N*(tx.T.dot(error))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        #  compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        #  update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*gradient\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return ws[-1], losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train dataset\n",
    "y, x, ids = load_csv_data(\"train.csv\")\n",
    "#x, _, _ = standardize(input_data)\n",
    "# input_data, _ = build_model_data(input_data, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999.000    177457\n",
       " 30.447         11\n",
       " 32.982         10\n",
       " 35.062          9\n",
       " 33.740          9\n",
       " 34.243          9\n",
       " 30.180          9\n",
       " 42.376          9\n",
       " 32.693          9\n",
       " 41.979          9\n",
       " 31.494          8\n",
       " 33.980          8\n",
       " 30.729          8\n",
       " 42.368          8\n",
       " 35.490          8\n",
       " 31.755          8\n",
       " 41.453          8\n",
       " 30.081          8\n",
       " 31.788          8\n",
       " 34.626          8\n",
       " 32.374          8\n",
       " 41.966          8\n",
       " 30.102          8\n",
       " 33.203          8\n",
       " 30.483          8\n",
       " 33.118          8\n",
       " 32.768          8\n",
       " 37.346          8\n",
       " 37.363          8\n",
       " 33.254          8\n",
       "             ...  \n",
       " 82.436          1\n",
       " 40.649          1\n",
       " 85.224          1\n",
       " 36.318          1\n",
       " 86.826          1\n",
       " 134.188         1\n",
       " 127.346         1\n",
       " 77.287          1\n",
       " 90.758          1\n",
       " 44.108          1\n",
       " 45.892          1\n",
       " 75.556          1\n",
       " 87.848          1\n",
       " 96.260          1\n",
       " 102.029         1\n",
       " 66.194          1\n",
       " 76.111          1\n",
       " 64.014          1\n",
       " 79.435          1\n",
       " 95.979          1\n",
       " 51.825          1\n",
       " 52.509          1\n",
       " 137.751         1\n",
       " 180.096         1\n",
       " 224.238         1\n",
       " 59.111          1\n",
       " 222.604         1\n",
       " 73.447          1\n",
       " 47.690          1\n",
       " 68.253          1\n",
       "Name: PRI_jet_subleading_pt, Length: 42464, dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk['PRI_jet_subleading_pt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.126690         1\n",
       " 4.073878         1\n",
       " 4.073183         1\n",
       " 3.960608         1\n",
       " 4.894559         1\n",
       " 3.334500         1\n",
       " 4.686088         1\n",
       " 4.593666         1\n",
       " 3.761171         1\n",
       " 3.852898         1\n",
       " 4.047471         1\n",
       " 3.806340         1\n",
       " 3.998828         1\n",
       " 4.578378         1\n",
       " 4.374076         1\n",
       " 4.822984         1\n",
       " 4.535294         1\n",
       " 4.254553         1\n",
       " 4.477617         1\n",
       " 4.241350         1\n",
       " 4.385195         1\n",
       " 4.804222         1\n",
       " 3.442210         1\n",
       " 4.969609         1\n",
       " 4.020370         1\n",
       " 3.563818         1\n",
       " 3.784798         1\n",
       " 4.686783         1\n",
       " 3.585360         1\n",
       " 4.308755         1\n",
       "              ...  \n",
       "-0.165037        29\n",
       "-0.219935        29\n",
       "-0.126123        30\n",
       " 0.037875        30\n",
       "-0.143495        30\n",
       " 0.017028        30\n",
       "-0.207427        30\n",
       "-0.001040        30\n",
       "-0.222714        30\n",
       "-0.042734        30\n",
       " 0.019807        30\n",
       "-0.101801        30\n",
       "-0.085123        30\n",
       "-0.224799        31\n",
       "-0.424237        31\n",
       "-0.472880        31\n",
       "-0.140021        31\n",
       "-0.445084        31\n",
       "-0.066361        31\n",
       "-0.039955        31\n",
       "-0.163648        31\n",
       "-0.201172        32\n",
       " 0.413818        32\n",
       "-0.085818        32\n",
       " 0.065671        32\n",
       "-0.114309        32\n",
       "-0.308188        32\n",
       "-0.184495        32\n",
       "-0.258155        33\n",
       "-0.484693    177463\n",
       "Name: DER_deltaeta_jet_jet, Length: 7086, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k1,_,_ = standardize(kk['DER_deltaeta_jet_jet'].replace(-999,0))\n",
    "k1.value_counts().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Since there are so many -999 values in a non-negative columns, these irreasonable values should be replaced by 0, which has much less influence on stardarize\n",
    "x[x==-999] = 0\n",
    "x, _, _ = standardize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 30) (125000, 30)\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.5\n",
    "x_train, x_test, y_train, y_test = split_data(x, y, ratio)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000, 31) (125000, 31)\n"
     ]
    }
   ],
   "source": [
    "degree = 1\n",
    "tx_train = build_poly(x_train, degree)\n",
    "tx_test = build_poly(x_test, degree)\n",
    "print(tx_train.shape, tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3394927804963772 0.33912978403528576\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(y_train, tx_train)\n",
    "print(loss, compute_loss(y_test, tx_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=0.5, w0=-0.13157919999999998, w1=0.015582450653986621\n",
      "Gradient Descent(1/99): loss=0.4253049237033968, w0=-0.10355199999999998, w1=0.018414518099978475\n",
      "Gradient Descent(2/99): loss=0.4934705940859776, w0=-0.10947679999999997, w1=0.015664293207585608\n",
      "Gradient Descent(3/99): loss=0.42840675270400036, w0=-0.10014559999999997, w1=0.016886519104186898\n",
      "Gradient Descent(4/99): loss=0.44989289874780714, w0=-0.08589439999999997, w1=0.015321148709328242\n",
      "Gradient Descent(5/99): loss=0.4228212764631573, w0=-0.07255679999999996, w1=0.01567709865760734\n",
      "Gradient Descent(6/99): loss=0.48991850593023556, w0=-0.09407839999999996, w1=0.01875987092861484\n",
      "Gradient Descent(7/99): loss=0.4294524292150715, w0=-0.08698719999999996, w1=0.018952711677077832\n",
      "Gradient Descent(8/99): loss=0.43264655361445975, w0=-0.07868319999999995, w1=0.014890554326148088\n",
      "Gradient Descent(9/99): loss=0.4261056843673296, w0=-0.05846559999999995, w1=0.01389137694481212\n",
      "Gradient Descent(10/99): loss=0.4866188793718067, w0=-0.07863039999999995, w1=0.012553620203112792\n",
      "Gradient Descent(11/99): loss=0.4384502640562398, w0=-0.07321759999999995, w1=0.01378565422429446\n",
      "Gradient Descent(12/99): loss=0.43440814234204095, w0=-0.06664479999999995, w1=0.011372991239410995\n",
      "Gradient Descent(13/99): loss=0.4343856085596754, w0=-0.04502719999999995, w1=0.010586370730906694\n",
      "Gradient Descent(14/99): loss=0.4918336753532074, w0=-0.07219039999999995, w1=0.014213630631782066\n",
      "Gradient Descent(15/99): loss=0.44513244007323427, w0=-0.06832639999999994, w1=0.015166428359255388\n",
      "Gradient Descent(16/99): loss=0.4259443283307146, w0=-0.06230879999999994, w1=0.0119518551781062\n",
      "Gradient Descent(17/99): loss=0.44588936652734246, w0=-0.04578079999999994, w1=0.009479190494773101\n",
      "Gradient Descent(18/99): loss=0.4503749562542545, w0=-0.02637439999999994, w1=0.00804476170965812\n",
      "Gradient Descent(19/99): loss=0.5156298264674131, w0=-0.07181919999999994, w1=0.027273757028568407\n",
      "Gradient Descent(20/99): loss=0.4505122352806623, w0=-0.06947359999999994, w1=0.026622207185642406\n",
      "Gradient Descent(21/99): loss=0.4143426594891469, w0=-0.06623679999999994, w1=0.01958928700211144\n",
      "Gradient Descent(22/99): loss=0.45662470762158514, w0=-0.06723999999999994, w1=0.012553598963360574\n",
      "Gradient Descent(23/99): loss=0.4311121453080512, w0=-0.05926079999999994, w1=0.013868166009627982\n",
      "Gradient Descent(24/99): loss=0.47990243982651193, w0=-0.07912959999999994, w1=0.014283781815938656\n",
      "Gradient Descent(25/99): loss=0.4299010679002155, w0=-0.07279839999999994, w1=0.015671625951735157\n",
      "Gradient Descent(26/99): loss=0.4354260316993244, w0=-0.07089599999999995, w1=0.01286464775653548\n",
      "Gradient Descent(27/99): loss=0.4250353544095164, w0=-0.05134079999999994, w1=0.012562147266634733\n",
      "Gradient Descent(28/99): loss=0.4899860782404144, w0=-0.07839999999999994, w1=0.01613444048710183\n",
      "Gradient Descent(29/99): loss=0.4336808852999271, w0=-0.07341599999999994, w1=0.01705865462952657\n",
      "Gradient Descent(30/99): loss=0.4243822795384138, w0=-0.06960479999999994, w1=0.013661133461024905\n",
      "Gradient Descent(31/99): loss=0.430398803157283, w0=-0.05071519999999994, w1=0.011475875422408743\n",
      "Gradient Descent(32/99): loss=0.46510112821233907, w0=-0.05502719999999994, w1=0.0068671449543255\n",
      "Gradient Descent(33/99): loss=0.43976306095370366, w0=-0.04982559999999994, w1=0.00933549532952212\n",
      "Gradient Descent(34/99): loss=0.4748341607422071, w0=-0.07011199999999994, w1=0.011164366561458427\n",
      "Gradient Descent(35/99): loss=0.43317281840074306, w0=-0.06459519999999994, w1=0.012753717305243594\n",
      "Gradient Descent(36/99): loss=0.44297587563930596, w0=-0.06639359999999994, w1=0.01005708104988291\n",
      "Gradient Descent(37/99): loss=0.42633856181668456, w0=-0.04921599999999994, w1=0.010792919321585966\n",
      "Gradient Descent(38/99): loss=0.4941270973439794, w0=-0.08105439999999994, w1=0.019141411276919672\n",
      "Gradient Descent(39/99): loss=0.4315507639598415, w0=-0.07638399999999994, w1=0.019486280381230753\n",
      "Gradient Descent(40/99): loss=0.41898124008213056, w0=-0.07263199999999993, w1=0.015369443253040776\n",
      "Gradient Descent(41/99): loss=0.42909548759754573, w0=-0.05625919999999993, w1=0.012359572758998025\n",
      "Gradient Descent(42/99): loss=0.45110662939873325, w0=-0.04804319999999993, w1=0.009079604490738705\n",
      "Gradient Descent(43/99): loss=0.4410966025132481, w0=-0.03383359999999993, w1=0.010524666996904537\n",
      "Gradient Descent(44/99): loss=0.5238751158299213, w0=-0.08025759999999993, w1=0.031064971747550844\n",
      "Gradient Descent(45/99): loss=0.43389146850678434, w0=-0.07702559999999993, w1=0.029624421026902912\n",
      "Gradient Descent(46/99): loss=0.41155192116710015, w0=-0.07523679999999994, w1=0.02275018415219117\n",
      "Gradient Descent(47/99): loss=0.42936935802289716, w0=-0.06309919999999994, w1=0.0182343188093124\n",
      "Gradient Descent(48/99): loss=0.4364448805660653, w0=-0.04386399999999994, w1=0.015253163060671433\n",
      "Gradient Descent(49/99): loss=0.47385216945860803, w0=-0.056993599999999936, w1=0.009388705664094181\n",
      "Gradient Descent(50/99): loss=0.4429625585703781, w0=-0.053446399999999936, w1=0.012132219666345296\n",
      "Gradient Descent(51/99): loss=0.447015573299504, w0=-0.05849599999999994, w1=0.01035227872316329\n",
      "Gradient Descent(52/99): loss=0.4316356159906789, w0=-0.044596799999999936, w1=0.012020630601553492\n",
      "Gradient Descent(53/99): loss=0.5061767644599742, w0=-0.08295839999999993, w1=0.026335934319080257\n",
      "Gradient Descent(54/99): loss=0.43097000234112526, w0=-0.07868959999999993, w1=0.02595576869446203\n",
      "Gradient Descent(55/99): loss=0.4137035359932743, w0=-0.07547519999999992, w1=0.020675648493765394\n",
      "Gradient Descent(56/99): loss=0.42698919696013193, w0=-0.061171199999999926, w1=0.016895279340381\n",
      "Gradient Descent(57/99): loss=0.4418682405187588, w0=-0.046675199999999924, w1=0.013876871104482555\n",
      "Gradient Descent(58/99): loss=0.44930288544818836, w0=-0.030193599999999925, w1=0.01191367512453525\n",
      "Gradient Descent(59/99): loss=0.4797062517392249, w0=-0.04925439999999993, w1=0.008501057763984684\n",
      "Gradient Descent(60/99): loss=0.44977843094941355, w0=-0.04705759999999993, w1=0.011632329697387794\n",
      "Gradient Descent(61/99): loss=0.4458156455264003, w0=-0.05252799999999993, w1=0.009970359660386138\n",
      "Gradient Descent(62/99): loss=0.4381727194996081, w0=-0.034761599999999934, w1=0.011844262175223667\n",
      "Gradient Descent(63/99): loss=0.517036966607542, w0=-0.07914559999999993, w1=0.03036399316410536\n",
      "Gradient Descent(64/99): loss=0.43450299819605714, w0=-0.07587999999999993, w1=0.029462912825070322\n",
      "Gradient Descent(65/99): loss=0.4120628748243325, w0=-0.07422079999999993, w1=0.02294696994675523\n",
      "Gradient Descent(66/99): loss=0.4296838453447229, w0=-0.06212159999999994, w1=0.018595398083182526\n",
      "Gradient Descent(67/99): loss=0.4365650889042799, w0=-0.04254239999999994, w1=0.015974337142513277\n",
      "Gradient Descent(68/99): loss=0.4748127666481228, w0=-0.057582399999999936, w1=0.010388796496356701\n",
      "Gradient Descent(69/99): loss=0.4424828074059956, w0=-0.05409439999999994, w1=0.013125386648059371\n",
      "Gradient Descent(70/99): loss=0.4451274426910293, w0=-0.059063999999999936, w1=0.011156334429854241\n",
      "Gradient Descent(71/99): loss=0.4308946766312412, w0=-0.044614399999999936, w1=0.012696035359840492\n",
      "Gradient Descent(72/99): loss=0.5052657731230541, w0=-0.08231679999999994, w1=0.02590396200406686\n",
      "Gradient Descent(73/99): loss=0.4304104674780662, w0=-0.07800799999999994, w1=0.025775161657893557\n",
      "Gradient Descent(74/99): loss=0.4146313599942969, w0=-0.07541599999999994, w1=0.020599760740102297\n",
      "Gradient Descent(75/99): loss=0.42564603379635724, w0=-0.06025759999999994, w1=0.0170388149196116\n",
      "Gradient Descent(76/99): loss=0.4451685685705373, w0=-0.05015679999999994, w1=0.013873620346090501\n",
      "Gradient Descent(77/99): loss=0.4396594927605607, w0=-0.033451199999999945, w1=0.014500730249864641\n",
      "Gradient Descent(78/99): loss=0.5175883419689268, w0=-0.07691839999999994, w1=0.030851691413125673\n",
      "Gradient Descent(79/99): loss=0.43446372710638675, w0=-0.07372319999999995, w1=0.030274218282610454\n",
      "Gradient Descent(80/99): loss=0.41371312875284905, w0=-0.07346879999999995, w1=0.02410626351222952\n",
      "Gradient Descent(81/99): loss=0.428364189833232, w0=-0.05979679999999995, w1=0.020010637507569\n",
      "Gradient Descent(82/99): loss=0.44102710323804256, w0=-0.04456799999999995, w1=0.017060084236366027\n",
      "Gradient Descent(83/99): loss=0.4546414856943024, w0=-0.03717439999999995, w1=0.014451669011915004\n",
      "Gradient Descent(84/99): loss=0.4497143592445019, w0=-0.023297599999999946, w1=0.015725059445745287\n",
      "Gradient Descent(85/99): loss=0.5352977995374768, w0=-0.07373119999999994, w1=0.0387014097388958\n",
      "Gradient Descent(86/99): loss=0.4353963888324318, w0=-0.07111199999999994, w1=0.037133652068705815\n",
      "Gradient Descent(87/99): loss=0.41351491782529187, w0=-0.07309119999999994, w1=0.029173565359297014\n",
      "Gradient Descent(88/99): loss=0.42785120968113444, w0=-0.059751999999999944, w1=0.024291129137986568\n",
      "Gradient Descent(89/99): loss=0.44180547553397753, w0=-0.045219199999999946, w1=0.021057968626936343\n",
      "Gradient Descent(90/99): loss=0.4517062622005522, w0=-0.033598399999999945, w1=0.018759208317244765\n",
      "Gradient Descent(91/99): loss=0.46322357282666493, w0=-0.03312799999999994, w1=0.01590608143697131\n",
      "Gradient Descent(92/99): loss=0.45080240542748784, w0=-0.025980799999999943, w1=0.01899159421663806\n",
      "Gradient Descent(93/99): loss=0.5467437337010994, w0=-0.07780799999999993, w1=0.04360288928949568\n",
      "Gradient Descent(94/99): loss=0.4303143573241088, w0=-0.07450239999999993, w1=0.04146357586025874\n",
      "Gradient Descent(95/99): loss=0.4153210365160971, w0=-0.07641599999999993, w1=0.03302762663477002\n",
      "Gradient Descent(96/99): loss=0.4221875532847276, w0=-0.06021599999999992, w1=0.028319545099405152\n",
      "Gradient Descent(97/99): loss=0.4553298341371576, w0=-0.06131679999999992, w1=0.022986500633694916\n",
      "Gradient Descent(98/99): loss=0.43301595302183954, w0=-0.05522399999999992, w1=0.02457077545411397\n",
      "Gradient Descent(99/99): loss=0.4763518767033346, w0=-0.07515359999999992, w1=0.021438792210805836\n",
      "0.4763518767033346 0.42812479468590386\n"
     ]
    }
   ],
   "source": [
    "#test GD()\n",
    "w_initial = np.zeros(tx_train.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.1\n",
    "w, loss = least_squares_GD(y_train, tx_train, w_initial, max_iters, gamma)\n",
    "print(loss, compute_loss(y_test, tx_test,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correct labels in training: 92845\n",
      "Training accuracy: 74.276\n",
      "Total correct labels in training: 92696\n",
      "Testing accuracy: 74.1568\n"
     ]
    }
   ],
   "source": [
    "training_predict_labels = calculate_predicted_labels(tx_train, w)\n",
    "testing_predict_labels = calculate_predicted_labels(tx_test, w)\n",
    "print_accuracy(training_predict_labels, tx_train, y_train)\n",
    "print_accuracy(testing_predict_labels, tx_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
