Training the model for category: 0
This is the gamma used: 0.001
################################################################
Build features for the poly-combination task: (99913, 18)
Build features for the poly-combination task: (227458, 18)
Build features for the poly task (train shape): (99913, 37)
Build features for the poly task (test shape): (227458, 37)
Loss (regularization) calculated at: 2.7900872869037534 , training step: 0
Loss (regularization) calculated at: 0.9725046713449783 , training step: 100
Loss (regularization) calculated at: 0.7331915127040509 , training step: 200
Loss (regularization) calculated at: 0.6215978316977699 , training step: 300
Loss (regularization) calculated at: 0.5522361367160549 , training step: 400
Loss (regularization) calculated at: 0.5047627267707175 , training step: 500
Loss (regularization) calculated at: 0.4712283684688239 , training step: 600
Loss (regularization) calculated at: 0.4467852375641064 , training step: 700
Loss (regularization) calculated at: 0.42856569712046727 , training step: 800
Loss (regularization) calculated at: 0.415004835758002 , training step: 900
Loss (regularization) calculated at: 0.404940725954472 , training step: 1000
Loss (regularization) calculated at: 0.3974301961678055 , training step: 1100
Loss (regularization) calculated at: 0.39175834254635455 , training step: 1200
Loss (regularization) calculated at: 0.3874264577054948 , training step: 1300
Loss (regularization) calculated at: 0.3840763711069452 , training step: 1400
Loss (regularization) calculated at: 0.381447626143287 , training step: 1500
Loss (regularization) calculated at: 0.3793505776827355 , training step: 1600
Loss (regularization) calculated at: 0.377648263152093 , training step: 1700
Loss (regularization) calculated at: 0.37624235013697505 , training step: 1800
Loss (regularization) calculated at: 0.3750622637253006 , training step: 1900
Loss (regularization) calculated at: 0.37405714859619593 , training step: 2000
Loss (regularization) calculated at: 0.3731900868914787 , training step: 2100
Loss (regularization) calculated at: 0.37243393669546987 , training step: 2200
Loss (regularization) calculated at: 0.3717680429973686 , training step: 2300
Loss (regularization) calculated at: 0.3711744851958121 , training step: 2400
Loss (regularization) calculated at: 0.37063671269506243 , training step: 2500
Loss (regularization) calculated at: 0.37014773537009565 , training step: 2600
Loss (regularization) calculated at: 0.3697045685205217 , training step: 2700
Loss (regularization) calculated at: 0.3693026120676241 , training step: 2800
Loss (regularization) calculated at: 0.36893747366573604 , training step: 2900
Loss (regularization) calculated at: 0.36860555657361443 , training step: 3000
Loss (regularization) calculated at: 0.3683039067287745 , training step: 3100
Loss (regularization) calculated at: 0.3680299212752051 , training step: 3200
Loss (regularization) calculated at: 0.36778098930273734 , training step: 3300
Loss (regularization) calculated at: 0.36755413112813223 , training step: 3400
Loss (regularization) calculated at: 0.36734660231461796 , training step: 3500
Loss (regularization) calculated at: 0.36715774362396886 , training step: 3600
Loss (regularization) calculated at: 0.366988015897563 , training step: 3700
Loss (regularization) calculated at: 0.36683685062324856 , training step: 3800
Loss (regularization) calculated at: 0.36670271519175796 , training step: 3900
Training accuracy: 83.69781710087776
The Average loss of train set: 0.36658486565235254
Shape of weight:  (37,)
Training for CATEGORY DONE: ########################################################
Training the model for category: 1
This is the gamma used: 0.001
################################################################
Build features for the poly-combination task: (77544, 253)
Build features for the poly-combination task: (175338, 253)
Build features for the poly task (train shape): (77544, 507)
Build features for the poly task (test shape): (175338, 507)
Loss (regularization) calculated at: 5.73026099337162 , training step: 0
Loss (regularization) calculated at: 3.293074171327706 , training step: 100
Loss (regularization) calculated at: 2.551782561589291 , training step: 200
Loss (regularization) calculated at: 2.1558828974286013 , training step: 300
Loss (regularization) calculated at: 1.8797664154481335 , training step: 400
Loss (regularization) calculated at: 1.6648914865905726 , training step: 500
Loss (regularization) calculated at: 1.5052561827503548 , training step: 600
Loss (regularization) calculated at: 1.3786291270603679 , training step: 700
Loss (regularization) calculated at: 1.2768176485527472 , training step: 800
Loss (regularization) calculated at: 1.1885932682412186 , training step: 900
Loss (regularization) calculated at: 1.1178965509396859 , training step: 1000
Loss (regularization) calculated at: 1.055405944713392 , training step: 1100
Loss (regularization) calculated at: 1.001234187840887 , training step: 1200
Loss (regularization) calculated at: 0.954451204699841 , training step: 1300
Loss (regularization) calculated at: 0.9139341951950524 , training step: 1400
Loss (regularization) calculated at: 0.8766314663360294 , training step: 1500
Loss (regularization) calculated at: 0.8428615840507307 , training step: 1600
Loss (regularization) calculated at: 0.8119430242640188 , training step: 1700
Loss (regularization) calculated at: 0.7840438025126735 , training step: 1800
Loss (regularization) calculated at: 0.7579153753193086 , training step: 1900
Loss (regularization) calculated at: 0.7353106673648715 , training step: 2000
Loss (regularization) calculated at: 0.7129331754511388 , training step: 2100
Loss (regularization) calculated at: 0.6928322255267549 , training step: 2200
Loss (regularization) calculated at: 0.6730844134938854 , training step: 2300
Loss (regularization) calculated at: 0.6581032719984866 , training step: 2400
Loss (regularization) calculated at: 0.6401637963199752 , training step: 2500
Loss (regularization) calculated at: 0.6264100353177884 , training step: 2600
Loss (regularization) calculated at: 0.611361112561224 , training step: 2700
Loss (regularization) calculated at: 0.5975945426801861 , training step: 2800
Loss (regularization) calculated at: 0.585527170172027 , training step: 2900
Loss (regularization) calculated at: 0.5744684564952128 , training step: 3000
Loss (regularization) calculated at: 0.5632931584161807 , training step: 3100
Loss (regularization) calculated at: 0.552911802938453 , training step: 3200
Loss (regularization) calculated at: 0.5438966852259048 , training step: 3300
Loss (regularization) calculated at: 0.5360354012482341 , training step: 3400
Loss (regularization) calculated at: 0.5276255586596859 , training step: 3500
Loss (regularization) calculated at: 0.5217117131840745 , training step: 3600
Loss (regularization) calculated at: 0.5139694739887644 , training step: 3700
Loss (regularization) calculated at: 0.5084731105133546 , training step: 3800
Loss (regularization) calculated at: 0.5039626151510246 , training step: 3900
Loss (regularization) calculated at: 0.49841034930028866 , training step: 4000
Loss (regularization) calculated at: 0.4939474434628834 , training step: 4100
Loss (regularization) calculated at: 0.4896741422647183 , training step: 4200
Loss (regularization) calculated at: 0.48593453943074083 , training step: 4300
Loss (regularization) calculated at: 0.4824485671585145 , training step: 4400
Loss (regularization) calculated at: 0.47947011133095463 , training step: 4500
Loss (regularization) calculated at: 0.477390863423939 , training step: 4600
Loss (regularization) calculated at: 0.47397184442825624 , training step: 4700
Loss (regularization) calculated at: 0.47194116287595766 , training step: 4800
Loss (regularization) calculated at: 0.4696922660930469 , training step: 4900
Loss (regularization) calculated at: 0.4671067064496854 , training step: 5000
Loss (regularization) calculated at: 0.4660822919573414 , training step: 5100
Loss (regularization) calculated at: 0.4628075884723768 , training step: 5200
Loss (regularization) calculated at: 0.46133786677312455 , training step: 5300
Loss (regularization) calculated at: 0.45905235417924833 , training step: 5400
Loss (regularization) calculated at: 0.4591856477211212 , training step: 5500
Loss (regularization) calculated at: 0.45729853615405597 , training step: 5600
Loss (regularization) calculated at: 0.4544068877493241 , training step: 5700
Loss (regularization) calculated at: 0.45389815284030727 , training step: 5800
Loss (regularization) calculated at: 0.4524537317466225 , training step: 5900
Loss (regularization) calculated at: 0.4507418493584655 , training step: 6000
Loss (regularization) calculated at: 0.4497228148643058 , training step: 6100
Loss (regularization) calculated at: 0.44845362782795417 , training step: 6200
Loss (regularization) calculated at: 0.4478969897877265 , training step: 6300
Loss (regularization) calculated at: 0.4464382691376223 , training step: 6400
Loss (regularization) calculated at: 0.4459842934962343 , training step: 6500
Loss (regularization) calculated at: 0.4451898434749624 , training step: 6600
Loss (regularization) calculated at: 0.4448599572578924 , training step: 6700
Loss (regularization) calculated at: 0.44368354333290805 , training step: 6800
Loss (regularization) calculated at: 0.44404872547975943 , training step: 6900
Loss (regularization) calculated at: 0.44332155792748024 , training step: 7000
Loss (regularization) calculated at: 0.4423645150569315 , training step: 7100
Loss (regularization) calculated at: 0.442155325093093 , training step: 7200
Loss (regularization) calculated at: 0.44139835599869814 , training step: 7300
Loss (regularization) calculated at: 0.4405517658792713 , training step: 7400
Loss (regularization) calculated at: 0.43956677343598866 , training step: 7500
Loss (regularization) calculated at: 0.4400188626430596 , training step: 7600
Loss (regularization) calculated at: 0.4392401706797141 , training step: 7700
Loss (regularization) calculated at: 0.4398485734389267 , training step: 7800
Loss (regularization) calculated at: 0.43850419745596264 , training step: 7900
Training accuracy: 80.85860930568451
The Average loss of train set: 0.4387246613582686
Shape of weight:  (507,)
Training for CATEGORY DONE: ########################################################
Training the model for category: 2
This is the gamma used: 0.0001
################################################################
Build features for the poly-combination task: (50379, 325)
Build features for the poly-combination task: (114648, 325)
Build features for the poly task (train shape): (50379, 651)
Build features for the poly task (test shape): (114648, 651)
Loss (regularization) calculated at: 6.002734074414299 , training step: 0
Loss (regularization) calculated at: 5.588318227890935 , training step: 100
Loss (regularization) calculated at: 5.375615302369059 , training step: 200
Loss (regularization) calculated at: 5.207739269335309 , training step: 300
Loss (regularization) calculated at: 5.0715711268794355 , training step: 400
Loss (regularization) calculated at: 4.950243849708622 , training step: 500
Loss (regularization) calculated at: 4.840497954347205 , training step: 600
Loss (regularization) calculated at: 4.7421269304463705 , training step: 700
Loss (regularization) calculated at: 4.647486564918058 , training step: 800
Loss (regularization) calculated at: 4.55731096444105 , training step: 900
Loss (regularization) calculated at: 4.472660711281409 , training step: 1000
Loss (regularization) calculated at: 4.391843780587579 , training step: 1100
Loss (regularization) calculated at: 4.316722372083058 , training step: 1200
Loss (regularization) calculated at: 4.247543006615793 , training step: 1300
Loss (regularization) calculated at: 4.175609828731848 , training step: 1400
Loss (regularization) calculated at: 4.101025642228084 , training step: 1500
Loss (regularization) calculated at: 4.03518852355432 , training step: 1600
Loss (regularization) calculated at: 3.9711201359179658 , training step: 1700
Loss (regularization) calculated at: 3.9103883354419255 , training step: 1800
Loss (regularization) calculated at: 3.8543333840112153 , training step: 1900
Loss (regularization) calculated at: 3.8000138246330413 , training step: 2000
Loss (regularization) calculated at: 3.747457143401961 , training step: 2100
Loss (regularization) calculated at: 3.696434891100956 , training step: 2200
Loss (regularization) calculated at: 3.647943610402153 , training step: 2300
Loss (regularization) calculated at: 3.601506519946551 , training step: 2400
Loss (regularization) calculated at: 3.556367921445223 , training step: 2500
Loss (regularization) calculated at: 3.509989568271045 , training step: 2600
Loss (regularization) calculated at: 3.4672085464170674 , training step: 2700
Loss (regularization) calculated at: 3.4243262712937685 , training step: 2800
Loss (regularization) calculated at: 3.3827472495273385 , training step: 2900
Loss (regularization) calculated at: 3.3410436747224033 , training step: 3000
Loss (regularization) calculated at: 3.301583970922414 , training step: 3100
Loss (regularization) calculated at: 3.2636488739706397 , training step: 3200
Loss (regularization) calculated at: 3.2267959466979663 , training step: 3300
Loss (regularization) calculated at: 3.191198176593095 , training step: 3400
Loss (regularization) calculated at: 3.1561584321807676 , training step: 3500
Loss (regularization) calculated at: 3.121232528314191 , training step: 3600
Loss (regularization) calculated at: 3.0876639207638803 , training step: 3700
Loss (regularization) calculated at: 3.0548350991723296 , training step: 3800
Loss (regularization) calculated at: 3.023887890422058 , training step: 3900
Loss (regularization) calculated at: 2.9932218526679644 , training step: 4000
Loss (regularization) calculated at: 2.96295301679092 , training step: 4100
Loss (regularization) calculated at: 2.933875399306263 , training step: 4200
Loss (regularization) calculated at: 2.905365130549447 , training step: 4300
Loss (regularization) calculated at: 2.876508117002513 , training step: 4400
Loss (regularization) calculated at: 2.849121762491832 , training step: 4500
Loss (regularization) calculated at: 2.8220647276754955 , training step: 4600
Loss (regularization) calculated at: 2.7954191815038567 , training step: 4700
Loss (regularization) calculated at: 2.769110110945551 , training step: 4800
Loss (regularization) calculated at: 2.7430045165436114 , training step: 4900
Loss (regularization) calculated at: 2.7178034723355418 , training step: 5000
Loss (regularization) calculated at: 2.6932082554716477 , training step: 5100
Loss (regularization) calculated at: 2.669404353771089 , training step: 5200
Loss (regularization) calculated at: 2.645824802811932 , training step: 5300
Loss (regularization) calculated at: 2.6219412417992896 , training step: 5400
Loss (regularization) calculated at: 2.598381387625651 , training step: 5500
Loss (regularization) calculated at: 2.5745879711104083 , training step: 5600
Loss (regularization) calculated at: 2.5502695668558655 , training step: 5700
Loss (regularization) calculated at: 2.5270133549285254 , training step: 5800
Loss (regularization) calculated at: 2.5038763357748217 , training step: 5900
Loss (regularization) calculated at: 2.481328345012119 , training step: 6000
Loss (regularization) calculated at: 2.4590672685842336 , training step: 6100
Loss (regularization) calculated at: 2.4368373942066843 , training step: 6200
Loss (regularization) calculated at: 2.4146639319659307 , training step: 6300
Loss (regularization) calculated at: 2.3931041854436783 , training step: 6400
Loss (regularization) calculated at: 2.3719981671884858 , training step: 6500
Loss (regularization) calculated at: 2.3507861561946486 , training step: 6600
Loss (regularization) calculated at: 2.330595696232871 , training step: 6700
Loss (regularization) calculated at: 2.3102760758552217 , training step: 6800
Loss (regularization) calculated at: 2.2897740323430287 , training step: 6900
Loss (regularization) calculated at: 2.269014255280774 , training step: 7000
Loss (regularization) calculated at: 2.2484161835361913 , training step: 7100
Loss (regularization) calculated at: 2.2279398111433006 , training step: 7200
Loss (regularization) calculated at: 2.206652384644781 , training step: 7300
Loss (regularization) calculated at: 2.1863813924620144 , training step: 7400
Loss (regularization) calculated at: 2.16595439853552 , training step: 7500
Loss (regularization) calculated at: 2.1454389148608004 , training step: 7600
Loss (regularization) calculated at: 2.125557826809979 , training step: 7700
Loss (regularization) calculated at: 2.1060783149668847 , training step: 7800
Loss (regularization) calculated at: 2.0875437821643206 , training step: 7900
Loss (regularization) calculated at: 2.068488195600025 , training step: 8000
Loss (regularization) calculated at: 2.049528946308361 , training step: 8100
Loss (regularization) calculated at: 2.030795679391541 , training step: 8200
Loss (regularization) calculated at: 2.0125579477945563 , training step: 8300
Loss (regularization) calculated at: 1.9942178701666542 , training step: 8400
Loss (regularization) calculated at: 1.9763695834535289 , training step: 8500
Loss (regularization) calculated at: 1.9587532992816927 , training step: 8600
Loss (regularization) calculated at: 1.9413497561710162 , training step: 8700
Loss (regularization) calculated at: 1.9233020690282827 , training step: 8800
Loss (regularization) calculated at: 1.9053083274140035 , training step: 8900
Loss (regularization) calculated at: 1.8874727829789126 , training step: 9000
Loss (regularization) calculated at: 1.870053692052995 , training step: 9100
Loss (regularization) calculated at: 1.8528651765552842 , training step: 9200
Loss (regularization) calculated at: 1.83574234796622 , training step: 9300
Loss (regularization) calculated at: 1.8186882338645434 , training step: 9400
Loss (regularization) calculated at: 1.8016363626674021 , training step: 9500
Loss (regularization) calculated at: 1.784809140390106 , training step: 9600
Loss (regularization) calculated at: 1.7684064953802896 , training step: 9700
Loss (regularization) calculated at: 1.751830857955336 , training step: 9800
Loss (regularization) calculated at: 1.735251847411171 , training step: 9900
Loss (regularization) calculated at: 1.718949660122171 , training step: 10000
Loss (regularization) calculated at: 1.7028158301660627 , training step: 10100
Loss (regularization) calculated at: 1.6866773638708026 , training step: 10200
Loss (regularization) calculated at: 1.6706003367556557 , training step: 10300
Loss (regularization) calculated at: 1.6544082592115175 , training step: 10400
Loss (regularization) calculated at: 1.6384922868516187 , training step: 10500
Loss (regularization) calculated at: 1.6227239609108672 , training step: 10600
Loss (regularization) calculated at: 1.6069624730621104 , training step: 10700
Loss (regularization) calculated at: 1.5914323204586307 , training step: 10800
Loss (regularization) calculated at: 1.5761951460547172 , training step: 10900
Loss (regularization) calculated at: 1.5613438547486798 , training step: 11000
Loss (regularization) calculated at: 1.54685286086003 , training step: 11100
Loss (regularization) calculated at: 1.5325649372143455 , training step: 11200
Loss (regularization) calculated at: 1.518436195667053 , training step: 11300
Loss (regularization) calculated at: 1.5043196542150508 , training step: 11400
Loss (regularization) calculated at: 1.4901522062078862 , training step: 11500
Loss (regularization) calculated at: 1.4755321889884814 , training step: 11600
Loss (regularization) calculated at: 1.461400489094132 , training step: 11700
Loss (regularization) calculated at: 1.4477859586217583 , training step: 11800
Loss (regularization) calculated at: 1.4343850126460003 , training step: 11900
Loss (regularization) calculated at: 1.4210337564200914 , training step: 12000
Loss (regularization) calculated at: 1.407679073028346 , training step: 12100
Loss (regularization) calculated at: 1.394359256483134 , training step: 12200
Loss (regularization) calculated at: 1.380691075697173 , training step: 12300
Loss (regularization) calculated at: 1.367532683261063 , training step: 12400
Loss (regularization) calculated at: 1.3546127870565818 , training step: 12500
Loss (regularization) calculated at: 1.3415984231796754 , training step: 12600
Loss (regularization) calculated at: 1.3286166139353088 , training step: 12700
Loss (regularization) calculated at: 1.3159251478885514 , training step: 12800
Loss (regularization) calculated at: 1.3037689572254159 , training step: 12900
Loss (regularization) calculated at: 1.2918571725164876 , training step: 13000
 Loss (regularization) calculated at: 1.2797719242051486 , training step: 13100
Loss (regularization) calculated at: 1.2679677625675034 , training step: 13200
Loss (regularization) calculated at: 1.2564358608940096 , training step: 13300
Loss (regularization) calculated at: 1.2449013642839952 , training step: 13400
Loss (regularization) calculated at: 1.2334126602878845 , training step: 13500
Loss (regularization) calculated at: 1.2219867735315926 , training step: 13600
Loss (regularization) calculated at: 1.2105132435191395 , training step: 13700
Loss (regularization) calculated at: 1.199123052432015 , training step: 13800
Loss (regularization) calculated at: 1.1878801911930796 , training step: 13900
Loss (regularization) calculated at: 1.1768096627879108 , training step: 14000
Loss (regularization) calculated at: 1.1659854898539792 , training step: 14100
Loss (regularization) calculated at: 1.1549456465836103 , training step: 14200
Loss (regularization) calculated at: 1.1440584136862606 , training step: 14300
Loss (regularization) calculated at: 1.1333688245082698 , training step: 14400
Loss (regularization) calculated at: 1.1231772422304747 , training step: 14500
Loss (regularization) calculated at: 1.112676746775907 , training step: 14600
Loss (regularization) calculated at: 1.1024197542599983 , training step: 14700
Loss (regularization) calculated at: 1.092340705283582 , training step: 14800
Loss (regularization) calculated at: 1.082465698079555 , training step: 14900
Loss (regularization) calculated at: 1.072836873156794 , training step: 15000
Loss (regularization) calculated at: 1.0631823830733917 , training step: 15100
Loss (regularization) calculated at: 1.0539188975690073 , training step: 15200
Loss (regularization) calculated at: 1.044608112767556 , training step: 15300
Loss (regularization) calculated at: 1.0353522409079143 , training step: 15400
Loss (regularization) calculated at: 1.0261678519448691 , training step: 15500
Loss (regularization) calculated at: 1.0171253768847222 , training step: 15600
Loss (regularization) calculated at: 1.00845955130794 , training step: 15700
Loss (regularization) calculated at: 0.9999313414095614 , training step: 15800
Loss (regularization) calculated at: 0.991331864253941 , training step: 15900
Loss (regularization) calculated at: 0.9826393092647483 , training step: 16000
Loss (regularization) calculated at: 0.9743226051702153 , training step: 16100
Loss (regularization) calculated at: 0.9661362267564013 , training step: 16200
Loss (regularization) calculated at: 0.9579546420347754 , training step: 16300
Loss (regularization) calculated at: 0.9497873352574442 , training step: 16400
Loss (regularization) calculated at: 0.9416048722282273 , training step: 16500
Loss (regularization) calculated at: 0.9337311954134232 , training step: 16600
Loss (regularization) calculated at: 0.926050981472218 , training step: 16700
Loss (regularization) calculated at: 0.9185402213300028 , training step: 16800
Loss (regularization) calculated at: 0.9112578812892594 , training step: 16900
Loss (regularization) calculated at: 0.904127983850798 , training step: 17000
Loss (regularization) calculated at: 0.8970212246746556 , training step: 17100
Loss (regularization) calculated at: 0.890067377720294 , training step: 17200
Loss (regularization) calculated at: 0.8831362712405003 , training step: 17300
Loss (regularization) calculated at: 0.8762038929954755 , training step: 17400
Loss (regularization) calculated at: 0.8693128394222313 , training step: 17500
Loss (regularization) calculated at: 0.8623108700419573 , training step: 17600
Loss (regularization) calculated at: 0.8549601734570523 , training step: 17700
Loss (regularization) calculated at: 0.8478443993286501 , training step: 17800
Loss (regularization) calculated at: 0.8409481699389124 , training step: 17900
Loss (regularization) calculated at: 0.8342873429033701 , training step: 18000
Loss (regularization) calculated at: 0.8277281254179595 , training step: 18100
Loss (regularization) calculated at: 0.8212359938703222 , training step: 18200
Loss (regularization) calculated at: 0.8149162104065021 , training step: 18300
Loss (regularization) calculated at: 0.8087570741648076 , training step: 18400
Loss (regularization) calculated at: 0.802603654841253 , training step: 18500
Loss (regularization) calculated at: 0.7965238545301097 , training step: 18600
Loss (regularization) calculated at: 0.7904859724818528 , training step: 18700
Loss (regularization) calculated at: 0.7844846826406976 , training step: 18800
Loss (regularization) calculated at: 0.7785314937673596 , training step: 18900
Loss (regularization) calculated at: 0.7726277291057211 , training step: 19000
Loss (regularization) calculated at: 0.766786635718098 , training step: 19100
Loss (regularization) calculated at: 0.7610286422724726 , training step: 19200
Loss (regularization) calculated at: 0.755390957762364 , training step: 19300
Loss (regularization) calculated at: 0.7498940854401828 , training step: 19400
Loss (regularization) calculated at: 0.7448400814685989 , training step: 19500
Loss (regularization) calculated at: 0.7393814318397169 , training step: 19600
Loss (regularization) calculated at: 0.7342097195329567 , training step: 19700
Loss (regularization) calculated at: 0.7290740662811824 , training step: 19800
Loss (regularization) calculated at: 0.7239990208308149 , training step: 19900
Loss (regularization) calculated at: 0.7188941251014976 , training step: 20000
Loss (regularization) calculated at: 0.7139158428092419 , training step: 20100
Loss (regularization) calculated at: 0.7090452778482818 , training step: 20200
Loss (regularization) calculated at: 0.7042718690874765 , training step: 20300
Loss (regularization) calculated at: 0.6995790808170237 , training step: 20400
Loss (regularization) calculated at: 0.694980263038854 , training step: 20500
Loss (regularization) calculated at: 0.6905094955037356 , training step: 20600
Loss (regularization) calculated at: 0.6861148204735641 , training step: 20700
Loss (regularization) calculated at: 0.6817721450594162 , training step: 20800
Loss (regularization) calculated at: 0.677477636812887 , training step: 20900
Loss (regularization) calculated at: 0.6732216823579749 , training step: 21000
Loss (regularization) calculated at: 0.6689951751244785 , training step: 21100
Loss (regularization) calculated at: 0.6651865521671663 , training step: 21200
Loss (regularization) calculated at: 0.660709109693597 , training step: 21300
Loss (regularization) calculated at: 0.6566590748001323 , training step: 21400
Loss (regularization) calculated at: 0.6526603306508483 , training step: 21500
Loss (regularization) calculated at: 0.6486668372233042 , training step: 21600
Loss (regularization) calculated at: 0.6446652027032872 , training step: 21700
Loss (regularization) calculated at: 0.6406775126281189 , training step: 21800
Loss (regularization) calculated at: 0.6367108000903909 , training step: 21900
Loss (regularization) calculated at: 0.6327812629982483 , training step: 22000
Loss (regularization) calculated at: 0.62889839819036 , training step: 22100
Loss (regularization) calculated at: 0.6250540164752534 , training step: 22200
Loss (regularization) calculated at: 0.6212874282697042 , training step: 22300
Loss (regularization) calculated at: 0.6176188231308781 , training step: 22400
Loss (regularization) calculated at: 0.6139848506718282 , training step: 22500
Loss (regularization) calculated at: 0.6103741530401396 , training step: 22600
Loss (regularization) calculated at: 0.6067653415222017 , training step: 22700
Loss (regularization) calculated at: 0.6031740030392204 , training step: 22800
Loss (regularization) calculated at: 0.5996021577465279 , training step: 22900
Loss (regularization) calculated at: 0.5960459534783383 , training step: 23000
Loss (regularization) calculated at: 0.5925047516344751 , training step: 23100
Loss (regularization) calculated at: 0.5889849530748347 , training step: 23200
Loss (regularization) calculated at: 0.58548713082297 , training step: 23300
Loss (regularization) calculated at: 0.5820287482954756 , training step: 23400
Loss (regularization) calculated at: 0.5785844778836463 , training step: 23500
Loss (regularization) calculated at: 0.5751682712416092 , training step: 23600
Loss (regularization) calculated at: 0.5719796651446389 , training step: 23700
Loss (regularization) calculated at: 0.5685954984691061 , training step: 23800
Loss (regularization) calculated at: 0.5653949739878867 , training step: 23900
Loss (regularization) calculated at: 0.5622074575522413 , training step: 24000
Loss (regularization) calculated at: 0.5590316912024133 , training step: 24100
Loss (regularization) calculated at: 0.555866995524612 , training step: 24200
Loss (regularization) calculated at: 0.5527164484583477 , training step: 24300
Loss (regularization) calculated at: 0.5495804662822918 , training step: 24400
Loss (regularization) calculated at: 0.5464548518013597 , training step: 24500
Loss (regularization) calculated at: 0.5433657813392683 , training step: 24600
Loss (regularization) calculated at: 0.5402885485364866 , training step: 24700
Loss (regularization) calculated at: 0.5372359295709785 , training step: 24800
Loss (regularization) calculated at: 0.5342199582023501 , training step: 24900
Loss (regularization) calculated at: 0.5312152492285868 , training step: 25000
Loss (regularization) calculated at: 0.5282581305258848 , training step: 25100
Loss (regularization) calculated at: 0.5253369144154306 , training step: 25200
Loss (regularization) calculated at: 0.5224485686304509 , training step: 25300
Loss (regularization) calculated at: 0.5196122706712955 , training step: 25400
Loss (regularization) calculated at: 0.5167873574739484 , training step: 25500
Loss (regularization) calculated at: 0.5140744985664919 , training step: 25600
Loss (regularization) calculated at: 0.5113593080022857 , training step: 25700
Loss (regularization) calculated at: 0.508686319897863 , training step: 25800
Loss (regularization) calculated at: 0.506042442761109 , training step: 25900
Loss (regularization) calculated at: 0.5034248460957301 , training step: 26000
Loss (regularization) calculated at: 0.500809252457026 , training step: 26100
Loss (regularization) calculated at: 0.4982146111372573 , training step: 26200
Loss (regularization) calculated at: 0.4956518531432101 , training step: 26300
Loss (regularization) calculated at: 0.49312638901607286 , training step: 26400
Loss (regularization) calculated at: 0.4906408286481681 , training step: 26500
Loss (regularization) calculated at: 0.4883385012164176 , training step: 26600
Loss (regularization) calculated at: 0.48592589216957033 , training step: 26700
Loss (regularization) calculated at: 0.48369176190144914 , training step: 26800
Loss (regularization) calculated at: 0.4815193109502701 , training step: 26900
Loss (regularization) calculated at: 0.47940293830093106 , training step: 27000
Loss (regularization) calculated at: 0.47732909848268795 , training step: 27100
Loss (regularization) calculated at: 0.47528586723411337 , training step: 27200
Loss (regularization) calculated at: 0.47327524275806726 , training step: 27300
Loss (regularization) calculated at: 0.47129775335106655 , training step: 27400
Loss (regularization) calculated at: 0.4693514555237415 , training step: 27500
Loss (regularization) calculated at: 0.4674306328690216 , training step: 27600
Loss (regularization) calculated at: 0.4655285741606847 , training step: 27700
Loss (regularization) calculated at: 0.46363578890164414 , training step: 27800
Loss (regularization) calculated at: 0.4621511475137719 , training step: 27900
Loss (regularization) calculated at: 0.45990943595256106 , training step: 28000
Loss (regularization) calculated at: 0.4580700503340005 , training step: 28100
Loss (regularization) calculated at: 0.4563137842179456 , training step: 28200
Loss (regularization) calculated at: 0.4545437441955633 , training step: 28300
Loss (regularization) calculated at: 0.45279304084201233 , training step: 28400
Loss (regularization) calculated at: 0.4510756530170883 , training step: 28500
Loss (regularization) calculated at: 0.4493754410843282 , training step: 28600
Loss (regularization) calculated at: 0.4476929579291991 , training step: 28700
Loss (regularization) calculated at: 0.4460343284822775 , training step: 28800
Loss (regularization) calculated at: 0.4444005942495024 , training step: 28900
Loss (regularization) calculated at: 0.4427895140858481 , training step: 29000
Loss (regularization) calculated at: 0.4411947950535005 , training step: 29100
Loss (regularization) calculated at: 0.43965439301593867 , training step: 29200
Loss (regularization) calculated at: 0.43821632570960234 , training step: 29300
Loss (regularization) calculated at: 0.4365706211167535 , training step: 29400
Loss (regularization) calculated at: 0.4351094455222202 , training step: 29500
Loss (regularization) calculated at: 0.4336769561640126 , training step: 29600
Loss (regularization) calculated at: 0.4322708611568754 , training step: 29700
Loss (regularization) calculated at: 0.43088513126048344 , training step: 29800
Loss (regularization) calculated at: 0.4295114535727107 , training step: 29900
Loss (regularization) calculated at: 0.4281487664213782 , training step: 30000
Loss (regularization) calculated at: 0.42679650765512733 , training step: 30100
Loss (regularization) calculated at: 0.42545606491809423 , training step: 30200
Loss (regularization) calculated at: 0.4242149775284972 , training step: 30300
Loss (regularization) calculated at: 0.42291600713760147 , training step: 30400
Loss (regularization) calculated at: 0.4217258134497495 , training step: 30500
Loss (regularization) calculated at: 0.4205406553316165 , training step: 30600
Loss (regularization) calculated at: 0.41938314060699855 , training step: 30700
Loss (regularization) calculated at: 0.41824018994770357 , training step: 30800
Loss (regularization) calculated at: 0.4171115235723399 , training step: 30900
Loss (regularization) calculated at: 0.41599013758640907 , training step: 31000
Loss (regularization) calculated at: 0.41487617568376295 , training step: 31100
Loss (regularization) calculated at: 0.4137698523530455 , training step: 31200
Loss (regularization) calculated at: 0.4126714634442921 , training step: 31300
Loss (regularization) calculated at: 0.4115861621805818 , training step: 31400
Loss (regularization) calculated at: 0.41051641501515573 , training step: 31500
Loss (regularization) calculated at: 0.4095478748991528 , training step: 31600
Loss (regularization) calculated at: 0.4085194359974649 , training step: 31700
Loss (regularization) calculated at: 0.407576164630939 , training step: 31800
Loss (regularization) calculated at: 0.4066470410457398 , training step: 31900
Loss (regularization) calculated at: 0.4057277035802368 , training step: 32000
Loss (regularization) calculated at: 0.4048215758342187 , training step: 32100
Loss (regularization) calculated at: 0.4039208812857258 , training step: 32200
Loss (regularization) calculated at: 0.4030263120986684 , training step: 32300
Loss (regularization) calculated at: 0.4021383134771769 , training step: 32400
Loss (regularization) calculated at: 0.40125717836530145 , training step: 32500
Loss (regularization) calculated at: 0.4003830821047862 , training step: 32600
Loss (regularization) calculated at: 0.3995244480488718 , training step: 32700
Loss (regularization) calculated at: 0.39876060325598706 , training step: 32800
Loss (regularization) calculated at: 0.3979285279511831 , training step: 32900
Loss (regularization) calculated at: 0.39717842271890197 , training step: 33000
Loss (regularization) calculated at: 0.3964369356001073 , training step: 33100
Loss (regularization) calculated at: 0.39570195628012667 , training step: 33200
Loss (regularization) calculated at: 0.3949731961331187 , training step: 33300
Loss (regularization) calculated at: 0.39425049622298153 , training step: 33400
Loss (regularization) calculated at: 0.39353375234240096 , training step: 33500
Loss (regularization) calculated at: 0.39282291879486686 , training step: 33600
Loss (regularization) calculated at: 0.39211801301083443 , training step: 33700
Loss (regularization) calculated at: 0.3914191295726098 , training step: 33800
Loss (regularization) calculated at: 0.39072645402061207 , training step: 33900
Loss (regularization) calculated at: 0.390257301886363 , training step: 34000
Loss (regularization) calculated at: 0.3895079290874468 , training step: 34100
Loss (regularization) calculated at: 0.38896059718617687 , training step: 34200
Loss (regularization) calculated at: 0.38841876627889677 , training step: 34300
Loss (regularization) calculated at: 0.3878832255579771 , training step: 34400
Loss (regularization) calculated at: 0.3873534356574968 , training step: 34500
Loss (regularization) calculated at: 0.38682902512257755 , training step: 34600
Loss (regularization) calculated at: 0.38630975951915214 , training step: 34700
Loss (regularization) calculated at: 0.3857955179909666 , training step: 34800
Loss (regularization) calculated at: 0.38528629246380386 , training step: 34900
Loss (regularization) calculated at: 0.38505633725004923 , training step: 35000
Loss (regularization) calculated at: 0.3845286954566775 , training step: 35100
Loss (regularization) calculated at: 0.3840068609865948 , training step: 35200
Loss (regularization) calculated at: 0.383665354958575 , training step: 35300
Loss (regularization) calculated at: 0.38348179541710214 , training step: 35400
Loss (regularization) calculated at: 0.38300399781854017 , training step: 35500
Loss (regularization) calculated at: 0.382697073075752 , training step: 35600
Loss (regularization) calculated at: 0.3823938315572659 , training step: 35700
Loss (regularization) calculated at: 0.38209339002113235 , training step: 35800
Loss (regularization) calculated at: 0.3817958084496738 , training step: 35900
Loss (regularization) calculated at: 0.38150272668811275 , training step: 36000
Loss (regularization) calculated at: 0.38121186164181103 , training step: 36100
Loss (regularization) calculated at: 0.38092285515727414 , training step: 36200
Loss (regularization) calculated at: 0.38063548534927777 , training step: 36300
Loss (regularization) calculated at: 0.3803493685319887 , training step: 36400
Loss (regularization) calculated at: 0.3800637098658294 , training step: 36500
Loss (regularization) calculated at: 0.3797770019208008 , training step: 36600
Loss (regularization) calculated at: 0.3794872513525208 , training step: 36700
Loss (regularization) calculated at: 0.3792030584683749 , training step: 36800
Loss (regularization) calculated at: 0.3789979806015652 , training step: 36900
Loss (regularization) calculated at: 0.37876197048501753 , training step: 37000
Loss (regularization) calculated at: 0.3786912545498656 , training step: 37100
Loss (regularization) calculated at: 0.3783945773552886 , training step: 37200
Loss (regularization) calculated at: 0.37818849721117026 , training step: 37300
Loss (regularization) calculated at: 0.3780044323923952 , training step: 37400
Loss (regularization) calculated at: 0.37783114387257066 , training step: 37500
Loss (regularization) calculated at: 0.37765489068023006 , training step: 37600
Loss (regularization) calculated at: 0.37747644011257425 , training step: 37700
Loss (regularization) calculated at: 0.3772958182036101 , training step: 37800
Loss (regularization) calculated at: 0.3771130906818091 , training step: 37900
Loss (regularization) calculated at: 0.37692834960897176 , training step: 38000
Loss (regularization) calculated at: 0.37674171396674305 , training step: 38100
Loss (regularization) calculated at: 0.3765533333989811 , training step: 38200
Loss (regularization) calculated at: 0.3763633949390356 , training step: 38300
Loss (regularization) calculated at: 0.37617357598060924 , training step: 38400
Loss (regularization) calculated at: 0.3760296268657367 , training step: 38500
Loss (regularization) calculated at: 0.37606135696767573 , training step: 38600
Loss (regularization) calculated at: 0.3757751494202437 , training step: 38700
Loss (regularization) calculated at: 0.37561127967584557 , training step: 38800
Loss (regularization) calculated at: 0.3755228062452251 , training step: 38900
Loss (regularization) calculated at: 0.37537156329659865 , training step: 39000
Loss (regularization) calculated at: 0.3752569011674752 , training step: 39100
Loss (regularization) calculated at: 0.37514112301551933 , training step: 39200
Loss (regularization) calculated at: 0.3750243742804256 , training step: 39300
Loss (regularization) calculated at: 0.3749065933502323 , training step: 39400
Loss (regularization) calculated at: 0.37478766894327603 , training step: 39500
Loss (regularization) calculated at: 0.3746675320229994 , training step: 39600
Loss (regularization) calculated at: 0.37454614942247705 , training step: 39700
Loss (regularization) calculated at: 0.3744235230659083 , training step: 39800
Loss (regularization) calculated at: 0.3742996961159456 , training step: 39900
Loss (regularization) calculated at: 0.37442075325269625 , training step: 40000
Loss (regularization) calculated at: 0.37410729219833205 , training step: 40100
Loss (regularization) calculated at: 0.37401873941202735 , training step: 40200
Loss (regularization) calculated at: 0.37393360760914013 , training step: 40300
Loss (regularization) calculated at: 0.37385445465885697 , training step: 40400
Loss (regularization) calculated at: 0.3737734658486236 , training step: 40500
Loss (regularization) calculated at: 0.373690742309593 , training step: 40600
Loss (regularization) calculated at: 0.37360633523331915 , training step: 40700
Loss (regularization) calculated at: 0.37352030558118043 , training step: 40800
Loss (regularization) calculated at: 0.37343272746038497 , training step: 40900
Loss (regularization) calculated at: 0.37334369034965603 , training step: 41000
Loss (regularization) calculated at: 0.37325329838863974 , training step: 41100
Loss (regularization) calculated at: 0.37316166895828784 , training step: 41200
Loss (regularization) calculated at: 0.3730850996646381 , training step: 41300
Loss (regularization) calculated at: 0.37299566698951986 , training step: 41400
Loss (regularization) calculated at: 0.37293225275334474 , training step: 41500
Loss (regularization) calculated at: 0.3728468084066612 , training step: 41600
Loss (regularization) calculated at: 0.3727741189681754 , training step: 41700
Loss (regularization) calculated at: 0.3727001111963108 , training step: 41800
Loss (regularization) calculated at: 0.37262495510609833 , training step: 41900
Loss (regularization) calculated at: 0.37254877247839185 , training step: 42000
Loss (regularization) calculated at: 0.37247170006048 , training step: 42100
Loss (regularization) calculated at: 0.37239388739033147 , training step: 42200
Loss (regularization) calculated at: 0.3723157668941614 , training step: 42300
Loss (regularization) calculated at: 0.3722594474087794 , training step: 42400
Loss (regularization) calculated at: 0.37223703571424377 , training step: 42500
Loss (regularization) calculated at: 0.3721631904936192 , training step: 42600
Loss (regularization) calculated at: 0.3721148510488651 , training step: 42700
Loss (regularization) calculated at: 0.37206553299433953 , training step: 42800
Loss (regularization) calculated at: 0.372015354241007 , training step: 42900
Loss (regularization) calculated at: 0.37196438453164044 , training step: 43000
Loss (regularization) calculated at: 0.3719127026112092 , training step: 43100
Loss (regularization) calculated at: 0.3718603936634729 , training step: 43200
Loss (regularization) calculated at: 0.37180755019375183 , training step: 43300
Loss (regularization) calculated at: 0.3717542723591894 , training step: 43400
Loss (regularization) calculated at: 0.37170066764721754 , training step: 43500
Loss (regularization) calculated at: 0.3716467071433833 , training step: 43600
Loss (regularization) calculated at: 0.37159305755043726 , training step: 43700
Loss (regularization) calculated at: 0.37157158173393934 , training step: 43800
Loss (regularization) calculated at: 0.3715253491750885 , training step: 43900
Loss (regularization) calculated at: 0.37149490602104485 , training step: 44000
Loss (regularization) calculated at: 0.37146392886266566 , training step: 44100
Loss (regularization) calculated at: 0.37143236591928064 , training step: 44200
Loss (regularization) calculated at: 0.3714002560159112 , training step: 44300
Loss (regularization) calculated at: 0.3713676491987641 , training step: 44400
Loss (regularization) calculated at: 0.3713345955348722 , training step: 44500
Loss (regularization) calculated at: 0.3713011464322911 , training step: 44600
Loss (regularization) calculated at: 0.3712673562568919 , training step: 44700
Loss (regularization) calculated at: 0.37123328302123904 , training step: 44800
Loss (regularization) calculated at: 0.3711989884558181 , training step: 44900
Training accuracy: 84.10250302705492
The Average loss of train set: 0.37125009475818427
Shape of weight:  (651,)
Training for CATEGORY DONE: ########################################################
Training the model for category: 3
This is the gamma used: 0.001
################################################################
Build features for the poly-combination task: (22164, 325)
Build features for the poly-combination task: (50794, 325)
Build features for the poly task (train shape): (22164, 651)
Build features for the poly task (test shape): (50794, 651)
Loss (regularization) calculated at: 5.6215671667387825 , training step: 0
Loss (regularization) calculated at: 3.2839641242318542 , training step: 100
Loss (regularization) calculated at: 2.7573841491796274 , training step: 200
Loss (regularization) calculated at: 2.420575333858116 , training step: 300
Loss (regularization) calculated at: 2.1362905032206023 , training step: 400
Loss (regularization) calculated at: 1.9235494683548482 , training step: 500
Loss (regularization) calculated at: 1.7556031230051485 , training step: 600
Loss (regularization) calculated at: 1.6206433616715148 , training step: 700
Loss (regularization) calculated at: 1.496896735526149 , training step: 800
Loss (regularization) calculated at: 1.3914583731510182 , training step: 900
Loss (regularization) calculated at: 1.2982668252615865 , training step: 1000
Loss (regularization) calculated at: 1.2137759436276023 , training step: 1100
Loss (regularization) calculated at: 1.1389810779828702 , training step: 1200
Loss (regularization) calculated at: 1.0743008002234407 , training step: 1300
Loss (regularization) calculated at: 1.0177303387789998 , training step: 1400
Loss (regularization) calculated at: 0.9669149666405255 , training step: 1500
Loss (regularization) calculated at: 0.9195266134569626 , training step: 1600
Loss (regularization) calculated at: 0.8764903325266076 , training step: 1700
Loss (regularization) calculated at: 0.8359070538912098 , training step: 1800
Loss (regularization) calculated at: 0.7980487168157531 , training step: 1900
Loss (regularization) calculated at: 0.7631539626131194 , training step: 2000
Loss (regularization) calculated at: 0.7303719178869365 , training step: 2100
Loss (regularization) calculated at: 0.6997825370163988 , training step: 2200
Loss (regularization) calculated at: 0.6719666950915162 , training step: 2300
Loss (regularization) calculated at: 0.6449533799132805 , training step: 2400
Loss (regularization) calculated at: 0.6195688695279605 , training step: 2500
Loss (regularization) calculated at: 0.5959931331281734 , training step: 2600
Loss (regularization) calculated at: 0.5737614530709173 , training step: 2700
Loss (regularization) calculated at: 0.5542884339254945 , training step: 2800
Loss (regularization) calculated at: 0.5375921678285704 , training step: 2900
Loss (regularization) calculated at: 0.5216810533982537 , training step: 3000
Loss (regularization) calculated at: 0.5084934823906335 , training step: 3100
Loss (regularization) calculated at: 0.49638911026477056 , training step: 3200
Loss (regularization) calculated at: 0.4860093305863232 , training step: 3300
Loss (regularization) calculated at: 0.476755277630795 , training step: 3400
Loss (regularization) calculated at: 0.4689097009233302 , training step: 3500
Loss (regularization) calculated at: 0.4618145197356983 , training step: 3600
Loss (regularization) calculated at: 0.45552392656654644 , training step: 3700
Loss (regularization) calculated at: 0.44977930721806764 , training step: 3800
Loss (regularization) calculated at: 0.44444403044167385 , training step: 3900
Loss (regularization) calculated at: 0.439586055295357 , training step: 4000
Loss (regularization) calculated at: 0.43520099065913453 , training step: 4100
Loss (regularization) calculated at: 0.43106698377735964 , training step: 4200
Loss (regularization) calculated at: 0.4273645401642133 , training step: 4300
Loss (regularization) calculated at: 0.4237713072194875 , training step: 4400
Loss (regularization) calculated at: 0.42044336394216236 , training step: 4500
Loss (regularization) calculated at: 0.4181420827320908 , training step: 4600
Loss (regularization) calculated at: 0.41497104257391354 , training step: 4700
Loss (regularization) calculated at: 0.41238902574364605 , training step: 4800
Loss (regularization) calculated at: 0.41004866807488405 , training step: 4900
Loss (regularization) calculated at: 0.4078420130212449 , training step: 5000
Loss (regularization) calculated at: 0.4058146655758657 , training step: 5100
Loss (regularization) calculated at: 0.403940592397841 , training step: 5200
Loss (regularization) calculated at: 0.40223579092954403 , training step: 5300
Loss (regularization) calculated at: 0.4005461920659686 , training step: 5400
Loss (regularization) calculated at: 0.39916661215408744 , training step: 5500
Loss (regularization) calculated at: 0.39763889665173363 , training step: 5600
Loss (regularization) calculated at: 0.3961693388903587 , training step: 5700
Loss (regularization) calculated at: 0.395355013874328 , training step: 5800
Loss (regularization) calculated at: 0.39378633287387305 , training step: 5900
Loss (regularization) calculated at: 0.3928530035266464 , training step: 6000
Loss (regularization) calculated at: 0.39182911306591756 , training step: 6100
Loss (regularization) calculated at: 0.39110448891350236 , training step: 6200
Loss (regularization) calculated at: 0.39019449086666885 , training step: 6300
Loss (regularization) calculated at: 0.38923717645855566 , training step: 6400
Loss (regularization) calculated at: 0.3886137898374463 , training step: 6500
Loss (regularization) calculated at: 0.38792687766057593 , training step: 6600
Loss (regularization) calculated at: 0.3873791500683722 , training step: 6700
Loss (regularization) calculated at: 0.38659039245931714 , training step: 6800
Loss (regularization) calculated at: 0.3861631199176238 , training step: 6900
Loss (regularization) calculated at: 0.3857368426929036 , training step: 7000
Loss (regularization) calculated at: 0.385125989556212 , training step: 7100
Loss (regularization) calculated at: 0.3847853081233739 , training step: 7200
Loss (regularization) calculated at: 0.38444937503373844 , training step: 7300
Loss (regularization) calculated at: 0.3842144477238961 , training step: 7400
Loss (regularization) calculated at: 0.38357438373073516 , training step: 7500
Loss (regularization) calculated at: 0.383246257242312 , training step: 7600
Loss (regularization) calculated at: 0.3829981451389437 , training step: 7700
Loss (regularization) calculated at: 0.38288280768932287 , training step: 7800
Loss (regularization) calculated at: 0.3824275903353503 , training step: 7900
Loss (regularization) calculated at: 0.382091740519309 , training step: 8000
Loss (regularization) calculated at: 0.381889841671079 , training step: 8100
Loss (regularization) calculated at: 0.38166851941894436 , training step: 8200
Loss (regularization) calculated at: 0.38171257103421113 , training step: 8300
Loss (regularization) calculated at: 0.3816925597384185 , training step: 8400
Loss (regularization) calculated at: 0.38135934337330696 , training step: 8500
Loss (regularization) calculated at: 0.3811933680439959 , training step: 8600
Loss (regularization) calculated at: 0.3809153242695403 , training step: 8700
Loss (regularization) calculated at: 0.38076675753477196 , training step: 8800
Loss (regularization) calculated at: 0.3806459767016684 , training step: 8900
Loss (regularization) calculated at: 0.3805303562696772 , training step: 9000
Loss (regularization) calculated at: 0.38115408027077624 , training step: 9100
Loss (regularization) calculated at: 0.38055918384699056 , training step: 9200
Loss (regularization) calculated at: 0.38029226855104553 , training step: 9300
Loss (regularization) calculated at: 0.3802046262706196 , training step: 9400
Loss (regularization) calculated at: 0.38012993645009874 , training step: 9500
Loss (regularization) calculated at: 0.38017069294526745 , training step: 9600
Loss (regularization) calculated at: 0.38023924817162913 , training step: 9700
Loss (regularization) calculated at: 0.3800266997611309 , training step: 9800
Loss (regularization) calculated at: 0.3800284217659863 , training step: 9900
Training accuracy: 84.26728027431871
The Average loss of train set: 0.37987280772144705
Shape of weight:  (651,)
Training for CATEGORY DONE: ########################################################
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
