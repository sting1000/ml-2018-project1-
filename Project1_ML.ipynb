{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will learn to use the concepts we have seen in the lectures and practiced in the labs on a real-world dataset, start to ﬁnish. You will do exploratory data analysis to understand your dataset and your features, do feature processing and engineering to clean your dataset and extract more meaningful information, implement and use machine learning methods on real data, analyze your model and generate predictions using those methods and report your ﬁndings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For raw data, firstly, we need chect what conponent it has and how can we deal with it. So we import panda library to have a quick view of train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>s</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>b</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100003</td>\n",
       "      <td>b</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100004</td>\n",
       "      <td>b</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "0  100000          s       138.470                       51.655        97.827   \n",
       "1  100001          b       160.937                       68.768       103.235   \n",
       "2  100002          b      -999.000                      162.172       125.953   \n",
       "3  100003          b       143.905                       81.417        80.943   \n",
       "4  100004          b       175.864                       16.915       134.805   \n",
       "\n",
       "   DER_pt_h  DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0    27.980                  0.91           124.711                2.666   \n",
       "1    48.146               -999.00          -999.000             -999.000   \n",
       "2    35.635               -999.00          -999.000             -999.000   \n",
       "3     0.414               -999.00          -999.000             -999.000   \n",
       "4    16.405               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep       ...        PRI_met_phi  PRI_met_sumet  \\\n",
       "0               3.064       ...             -0.277        258.733   \n",
       "1               3.473       ...             -1.916        164.546   \n",
       "2               3.148       ...             -2.186        260.414   \n",
       "3               3.310       ...              0.060         86.062   \n",
       "4               3.891       ...             -0.871         53.131   \n",
       "\n",
       "   PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  PRI_jet_leading_phi  \\\n",
       "0            2              67.435                2.150                0.444   \n",
       "1            1              46.226                0.725                1.158   \n",
       "2            1              44.251                2.053               -2.028   \n",
       "3            0            -999.000             -999.000             -999.000   \n",
       "4            0            -999.000             -999.000             -999.000   \n",
       "\n",
       "   PRI_jet_subleading_pt  PRI_jet_subleading_eta  PRI_jet_subleading_phi  \\\n",
       "0                 46.062                    1.24                  -2.475   \n",
       "1               -999.000                 -999.00                -999.000   \n",
       "2               -999.000                 -999.00                -999.000   \n",
       "3               -999.000                 -999.00                -999.000   \n",
       "4               -999.000                 -999.00                -999.000   \n",
       "\n",
       "   PRI_jet_all_pt  \n",
       "0         113.497  \n",
       "1          46.226  \n",
       "2          44.251  \n",
       "3           0.000  \n",
       "4           0.000  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"train.csv\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999.000    177457\n",
       " 30.447         11\n",
       " 32.982         10\n",
       " 35.062          9\n",
       " 33.740          9\n",
       "Name: PRI_jet_subleading_pt, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"train.csv\")['PRI_jet_subleading_pt'].value_counts().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000     99913\n",
       "36.358       10\n",
       "30.363       10\n",
       "36.493       10\n",
       "30.763       10\n",
       "Name: PRI_jet_all_pt, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"train.csv\")['PRI_jet_all_pt'].value_counts().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through observing the dataset, we find out that there are some positive columns has a large number of -999 which should be NaN actually. Specially, in one column called 'PRI_jet_all_pt', 0 value present quite frequently. All these situation above should be trated and give a reaonable solution.\n",
    "\n",
    "Here is our plan: \n",
    "1. Data eaquls to -999 is replaced by 0\n",
    "2. Consider 'PRI_jet_all_pt' as a normal case because we take 0 for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train dataset\n",
    "y, x, ids = load_csv_data(\"train.csv\")\n",
    "\n",
    "#replace -999 with 0\n",
    "x[x==-999] = 0\n",
    "\n",
    "# Standardizing it by features\n",
    "x, _, _ = standardize(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, correlation features should be take into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data and Basic Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the cell below to control all the parameters we need, so that all processes could be easy to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build polynomial by degree\n",
    "degree = 1\n",
    "#split train dataset to 2 parts for test and train\n",
    "ratio_split = 0.8\n",
    "#L2 penalty parameter for ridge_regression()\n",
    "lambda_ = 0.1\n",
    "#GD\n",
    "max_iters_GD = 100\n",
    "gamma_GD = 0.05\n",
    "#SGD\n",
    "max_iters_SGD = 100\n",
    "gamma_SGD = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build Polinomial and Split Data to train and test sets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = build_poly(x, degree)\n",
    "x_train, x_test, y_train, y_test = split_data(x_, y, ratio_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of x_train: (200000, 31)\n",
      "The size of x_test: (50000, 31)\n"
     ]
    }
   ],
   "source": [
    "print('The size of x_train: {}\\nThe size of x_test: {}'.format( x_train.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test the functions in implements.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least_squares()\n",
    "w, loss = least_squares(y_train, x_train)\n",
    "print(loss, compute_loss(y_test, x_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge_regression()\n",
    "w, loss = ridge_regression(y_train, x_train, lambda_)\n",
    "print(loss, compute_loss(y_test, x_test, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD()\n",
    "w_initial = np.zeros(x_train.shape[1])\n",
    "w, loss = least_squares_GD(y_train, x_train, w_initial, max_iters_GD, gamma_GD)\n",
    "print(loss, compute_loss(y_test,x_test,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD()\n",
    "w_initial = np.zeros(x_train.shape[1])\n",
    "\n",
    "# loss_mae is the argument to get the mean absolute error cost function running\n",
    "w, loss = least_squares_SGD(y_train, x_train, w_initial, max_iters_SGD, gamma_SGD)#, loss_function='rmse')\n",
    "# print('Training loss: {}'.format(loss))\n",
    "# print('Testing loss: {}'.format(compute_loss(y_test, x_test, w, loss_function='rmse')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss calculated at: 0.6931471805599453 , training step: 0\n",
      "Loss calculated at: 0.6918852030808587 , training step: 1\n",
      "Loss calculated at: 0.6906251899155496 , training step: 2\n",
      "Loss calculated at: 0.6893671349679705 , training step: 3\n",
      "Loss calculated at: 0.6881110321563456 , training step: 4\n",
      "Loss calculated at: 0.6868568754132587 , training step: 5\n",
      "Loss calculated at: 0.6856046586857383 , training step: 6\n",
      "Loss calculated at: 0.6843543759353434 , training step: 7\n",
      "Loss calculated at: 0.6831060211382439 , training step: 8\n",
      "Loss calculated at: 0.6818595882853028 , training step: 9\n",
      "Loss calculated at: 0.6806150713821528 , training step: 10\n",
      "Loss calculated at: 0.679372464449276 , training step: 11\n",
      "Loss calculated at: 0.6781317615220788 , training step: 12\n",
      "Loss calculated at: 0.6768929566509637 , training step: 13\n",
      "Loss calculated at: 0.6756560439014052 , training step: 14\n",
      "Loss calculated at: 0.6744210173540163 , training step: 15\n",
      "Loss calculated at: 0.6731878711046198 , training step: 16\n",
      "Loss calculated at: 0.6719565992643151 , training step: 17\n",
      "Loss calculated at: 0.6707271959595434 , training step: 18\n",
      "Loss calculated at: 0.6694996553321518 , training step: 19\n",
      "Loss calculated at: 0.6682739715394556 , training step: 20\n",
      "Loss calculated at: 0.6670501387542982 , training step: 21\n",
      "Loss calculated at: 0.6658281511651125 , training step: 22\n",
      "Loss calculated at: 0.6646080029759756 , training step: 23\n",
      "Loss calculated at: 0.6633896884066666 , training step: 24\n",
      "Loss calculated at: 0.6621732016927199 , training step: 25\n",
      "Loss calculated at: 0.6609585370854794 , training step: 26\n",
      "Loss calculated at: 0.6597456888521483 , training step: 27\n",
      "Loss calculated at: 0.6585346512758393 , training step: 28\n",
      "Loss calculated at: 0.6573254186556237 , training step: 29\n",
      "Loss calculated at: 0.6561179853065762 , training step: 30\n",
      "Loss calculated at: 0.6549123455598224 , training step: 31\n",
      "Loss calculated at: 0.6537084937625807 , training step: 32\n",
      "Loss calculated at: 0.6525064242782055 , training step: 33\n",
      "Loss calculated at: 0.6513061314862277 , training step: 34\n",
      "Loss calculated at: 0.6501076097823939 , training step: 35\n",
      "Loss calculated at: 0.6489108535787043 , training step: 36\n",
      "Loss calculated at: 0.6477158573034495 , training step: 37\n",
      "Loss calculated at: 0.6465226154012452 , training step: 38\n",
      "Loss calculated at: 0.6453311223330663 , training step: 39\n",
      "Loss calculated at: 0.6441413725762778 , training step: 40\n",
      "Loss calculated at: 0.6429533606246677 , training step: 41\n",
      "Loss calculated at: 0.641767080988475 , training step: 42\n",
      "Loss calculated at: 0.6405825281944183 , training step: 43\n",
      "Loss calculated at: 0.6393996967857223 , training step: 44\n",
      "Loss calculated at: 0.6382185813221445 , training step: 45\n",
      "Loss calculated at: 0.6370391763799976 , training step: 46\n",
      "Loss calculated at: 0.6358614765521738 , training step: 47\n",
      "Loss calculated at: 0.6346854764481655 , training step: 48\n",
      "Loss calculated at: 0.6335111706940862 , training step: 49\n",
      "Loss calculated at: 0.6323385539326891 , training step: 50\n",
      "Loss calculated at: 0.6311676208233855 , training step: 51\n",
      "Loss calculated at: 0.6299983660422603 , training step: 52\n",
      "Loss calculated at: 0.6288307842820888 , training step: 53\n",
      "Loss calculated at: 0.62766487025235 , training step: 54\n",
      "Loss calculated at: 0.6265006186792392 , training step: 55\n",
      "Loss calculated at: 0.6253380243056809 , training step: 56\n",
      "Loss calculated at: 0.624177081891338 , training step: 57\n",
      "Loss calculated at: 0.6230177862126232 , training step: 58\n",
      "Loss calculated at: 0.6218601320627046 , training step: 59\n",
      "Loss calculated at: 0.6207041142515162 , training step: 60\n",
      "Loss calculated at: 0.6195497276057612 , training step: 61\n",
      "Loss calculated at: 0.6183969669689183 , training step: 62\n",
      "Loss calculated at: 0.6172458272012454 , training step: 63\n",
      "Loss calculated at: 0.6160963031797828 , training step: 64\n",
      "Loss calculated at: 0.6149483897983543 , training step: 65\n",
      "Loss calculated at: 0.6138020819675685 , training step: 66\n",
      "Loss calculated at: 0.6126573746148181 , training step: 67\n",
      "Loss calculated at: 0.6115142626842789 , training step: 68\n",
      "Loss calculated at: 0.6103727411369075 , training step: 69\n",
      "Loss calculated at: 0.6092328049504384 , training step: 70\n",
      "Loss calculated at: 0.6080944491193786 , training step: 71\n",
      "Loss calculated at: 0.6069576686550039 , training step: 72\n",
      "Loss calculated at: 0.605822458585351 , training step: 73\n",
      "Loss calculated at: 0.6046888139552122 , training step: 74\n",
      "Loss calculated at: 0.6035567298261266 , training step: 75\n",
      "Loss calculated at: 0.6024262012763704 , training step: 76\n",
      "Loss calculated at: 0.6012972234009486 , training step: 77\n",
      "Loss calculated at: 0.6001697913115828 , training step: 78\n",
      "Loss calculated at: 0.5990439001367015 , training step: 79\n",
      "Loss calculated at: 0.5979195450214253 , training step: 80\n",
      "Loss calculated at: 0.5967967211275562 , training step: 81\n",
      "Loss calculated at: 0.5956754236335612 , training step: 82\n",
      "Loss calculated at: 0.5945556477345592 , training step: 83\n",
      "Loss calculated at: 0.5934373886423043 , training step: 84\n",
      "Loss calculated at: 0.59232064158517 , training step: 85\n",
      "Loss calculated at: 0.5912054018081304 , training step: 86\n",
      "Loss calculated at: 0.5900916645727448 , training step: 87\n",
      "Loss calculated at: 0.5889794251571362 , training step: 88\n",
      "Loss calculated at: 0.587868678855974 , training step: 89\n",
      "Loss calculated at: 0.5867594209804525 , training step: 90\n",
      "Loss calculated at: 0.58565164685827 , training step: 91\n",
      "Loss calculated at: 0.5845453518336073 , training step: 92\n",
      "Loss calculated at: 0.5834405312671062 , training step: 93\n",
      "Loss calculated at: 0.5823371805358448 , training step: 94\n",
      "Loss calculated at: 0.5812352950333158 , training step: 95\n",
      "Loss calculated at: 0.5801348701694 , training step: 96\n",
      "Loss calculated at: 0.5790359013703431 , training step: 97\n",
      "Loss calculated at: 0.5779383840787293 , training step: 98\n",
      "Loss calculated at: 0.5768423137534554 , training step: 99\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(y, tx, max_iters, gamma):\n",
    "    initial_w = np.zeros(tx.shape[1])\n",
    "    divide_by_constant = 1 / y.shape[0]\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        h = sigmoid(np.dot(initial_w, tx.T))\n",
    "        gradient = divide_by_constant * np.dot(tx.T, (h - y))\n",
    "        initial_w -= gamma * gradient\n",
    "        \n",
    "        loss = calculate_loss_logistic(h, y)\n",
    "\n",
    "        print(\n",
    "            'Loss calculated at: {} , training step: {}'.format(\n",
    "                 loss, n_iter\n",
    "            )\n",
    "        )\n",
    "    return initial_w, loss\n",
    "        \n",
    "def calculate_loss_logistic(h, y):\n",
    "    \"\"\"\n",
    "    Given the actual label y and calculated hypothesis h returns the loss\n",
    "    accumulated over all data points.\n",
    "    \"\"\"\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "max_iters_logistic = 100\n",
    "lr = 0.001\n",
    "w, loss = logistic_regression(y_train, x_train, max_iters_logistic, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predict_labels = calculate_predicted_labels(x_train, w)\n",
    "testing_predict_labels = calculate_predicted_labels(x_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy(training_predict_labels, x_train, y_train)\n",
    "print_accuracy(testing_predict_labels, x_test, y_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
